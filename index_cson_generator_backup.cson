createdAt: "2022-04-19T14:21:59.567Z"
updatedAt: "2022-04-27T13:41:19.678Z"
type: "MARKDOWN_NOTE"
folder: "36748f63b14206359257"
title: "McKenzieLab Repository, Matlab code"
tags: [
  "mckenzielab-gh-pages"
]
content: '''
  # McKenzieLab Repository, Matlab code
  4/19/2022, 3:22:08 PM
  
  This document attempts to provide the most-concise-as-possible outline of the codebase that would enable someone to re-write it from scratch. 
  
  The pipeline processes each session as follows (1 session = 1 raw edf + 1 seizure metadata text file). 
  
  ![simplified flowchart](/Users/steve/boostnote/pics/unm/simplified_flowchart_mlab.png)
  
  It we add the parameters defined inside scripts, we get the following.
  
  ![flowchart with io](/Users/steve/boostnote/pics/unm/io_flowchart_mlab.png)
  
  **Color Scheme**
  <fold>directory ("folder")</fold>
  <file>file</file>
  <mldep>MatLab dep</mldep>
  <heldep>helpers dependency</heldep>
  <buzdep>buzzcode dependency</buzdep>
  <smdep>"sm" / core IHKA pipeline file</smdep>
  <var>variable</var>
  
  ## Sketch of IHKA pipeline
  
  <smdep>sm_MakeAll_getPowerPerChannel</smdep> computes wavelet transforms and saves to disk as `.dat` file. 
  - Finds all <file>raw.edf</file> and <file>raw.txt</file> pairs inside of <fold>raw_data</fold>
  - For each (<file>patient_xyz_raw.txt</file>, <file>patient_xyz_raw.edf</file>) filepair, calls <smdep>sm_getPowerPerChannel</smdep> on each channel
    - args: takes filename <file>patient_xyz_raw.edf</file>, channel number (e.g. 3), and <var>varargin</var> (optional args)
    - computes wavelet decomp for 20 freqs, Power (/Amplitude) and Phase
    - saves 40 (=2x20) intermediate files in a temporary cache
    - calls <heldep>sm_MergDats<heldep>
    - combines intermediate files and saves as <file>patient_xyz.dat</file>
  
  <smdep>sm_PredictIHKA_getAllFeatures</smdep>
  - reads from <fold>raw/data/path</fold> only for <file>patient_xyz_raw.txt</file> files, and from <fold>wavelet/power/path</fold> for <file>patient_xyz.dat</file>
  - <var>ops.bins, ops.pct</var> defines prediction time bins and % of samples to chose from these bins in training resp.
  - <var>sessions</var> stores list of tuples (<file>patient_xyz.txt , patient_xyz.dat</file>)
  - <var>tims</var> empty list of matrices, <var>tims{i}</var> $N\\times$$M$ where $N$ is the number of seizures in the $i$'th componant and $M$ is the number of bins + 2 (one extra for the intraictal timestamp, one for postictal timestamp)
  - loop: for each session
    - for each preictal time bin
      - call <buzdep>getXPctTim</buzdep> to sample timestamps
      - for each timestamp 
        - compute the features with <smdep>sm_PredictIHKA_calcFeatures<smdep>
  - outputs a features matrix <file>features.mat</file> with 'save'—a MatLab function (saves specified workspace variables)
  
  [from above] <smdep>sm_PredictIHKA_calcFeatures</smdep> (it's a function)
  - takes: <var>fname</var> is the path to the feature file, <var>tim</var> is <var>tims{i}</var> from earlier, <var>ops</var> is a struct with all other options and metadata. 
  - for each channel
    - use <buzdep>LoadBinary</buzdep> to load a <var>ops.durFeat</var> (5 second) long time segment of each freq channel before the starting times specified by <var>tim</var>
    - save the <var>features</var>
      - mean power
      - whole time series in window?? (5 x 2000 dta points)
      - phase-amplitude correlation
      - coherence between every pair of features (n(n-1)/2 = 6)
  - returns <var>features</var>
  
  <smdep>sm_PredictIHKA</smdep>
  - loads a feature file to train the RUSboost model to predict which time bin the data originated from relative to the nearest seizure
  - loads features from <file>features.mat</file>
  - train/val/test - split
  - serializes model to <file>classification.mat</file>
  
  <smdep>sm_MakeAllSeizurePred</smdep>
  - this function takes a trained model with accompanying feature definition and calculates the predicted time to seizure. pulls data from both the raw time series, and the pre-calculated feature space
  - loads model <file>classification.mat</file> and features <file>features.mat</file>
  - loop over all sessions
    - call <smdep>sm_getSeizurePred</smdep>
      - this function takes the classifier in rusTree and applies the feature space specified in ops and classified ever moment in time for the file (fname)
      - 
    - save predictions <file>predict.mat</file>
  
  
  ## What are all the `ops`?
  *Color code*
  -> **BLACK** is for options defined in <smdep>sm_PredictIHKA_getAllFeatures.m</smdep>
  -> <red>**RED** is for options defined in <smdep>sm_PredictIHKA.m</smdep>, these options are saved with the model `classifier.mat`. These options are then loaded into <smdep>sm_MakeAllSeizurePred</smdep> and passed as an argument to <smdep>sm_getSeizurePred</smdep></red>
  
  
  ***File IO***
  - **ops.RawDataPath** Path to the directory where the raw `.edf` data files are stored.
  - **ops.FeaturePath** Path to directory where `.dat` files are stored. 
  - **ops.FeatureFileOutput** Path to the feature file `features.mat`.
  - <red>**ops.ClassifierFileOutput**</red> Path to serialized model `classification.mat`. 
  
  ***Hyper parameters***
  - **ops.durFeat** Window length in seconds
  - **ops.bins** Prediction time bins, e.g. `[10800 3600 600 10]` (i.e. 3h, 1h, 10min, 10sec in seconds)
  - **ops.pct** The ratio at which to sample data from each of the time bins `ops.bins`, e.g. `[.05 .05 .2 1 1 .2]`. The length is `len(ops.bins) + 2`, the extra two on the end correspond to the intra-ictal and post-ictal sampling ratios. Mnemonic: "percent".
  - **ops.nCh_featureFile** is this the number of frequency channels in the wavelet transform (-> hyper param) or the number of electrode channels (-> data param)?
  - **ops.nBins** Length of `ops.pct` which is 2 greater than the length of `ops.bins`. Comment: think of cleaner way to encapsulate `pct` and `bins`. 
  - **ops.timPost** Defines the interval of time in seconds between seizure offset and official commencement of post-ictal time. e.g. `600` *Note to self: verify this definition, especially the 'after seizure offset' part; what is the seizure offset, did I guess it right?*
  - **ops.freqs** The frequencies of the wavelets chosen for convolution (in Hz?). Logspace is more informative. e.g. `logspace(log10(.5),log10(200),20`. 
  - **ops.durFeat** Duration in seconds of windows used by model and in featurizing. e.g. `5`
  - **ops.nCh_featureFile** Number of channels in the `.dat` files. e.g. `41`—this number comes from 20 power, 20 phase, 1 time series
  
  ***Data Parameters***
  - **ops.Fs** Sample rate, in Hz. e.g. `2000`
  - **ops.amp_idx** indices of selected selected amplitude (power). e.g. `15:20` *Why are only a subset used? There is a comment that says "must match index in getPowerPerChannel"*
  - **ops.ph_idx** indices of selected phase e.g. `1:10`. There is a comment that says "must match index in getPowerPerChannel"
  - **ops.reScalePhase** e.g. `1000` *What does this do, why 1000?*
  - **ops.reScalePower** e.g. `1000` *What does this do, why 1000?*
  - **ops.nCh_raw** Number of hardware (electrode) channels. e.g. `4` in the case of the mouse.
  - **ops.ch_phaseAmp** Todo with the channel for phase amplitude coupling. `=2` (see comment below)
  
  ***Training Parameters***
  - <red>**ops.N**</red> (int) The number of observations in the training sample. Currently this is set to half the length of `training`. 
  - <red>**ops.t**</red> A template decision tree. e.g. `templateTree('MaxNumSplits',ops.N)`, [templateTree](https://www.mathworks.com/help/stats/templatetree.html) is a MatLab built-in.
  - <red>**ops.NumLearningCycles**</red> e.g. `500`
  - <red>**ops.Learners**</red> `= ops.t`
  - <red>**ops.LearnRate**</red> `=0.1`
  - <red>**ops.Method**</red> `'RUSBoost'`
  
  
  
  ***Misc***
  - **ops.features** This variable binds to the function <smdep>sm_PredictIHKA_calcFeatures</smdep>
  - <red>**ops.nGroup**</red> is defined as `length(dat)`, where `dat` is some global variable containing some form of data *What data? I figured it out at some point but forgot... TODO, next time you figure it out write the information here!*
  - <red>**ops.rix**</red> A random sample of indices for training sample. *In the code I think it's just a re-arrangement of all the indices for group. Only used to shuffle the group variable. `group` is some function of `dat`*
  
  
  *There is a comment that reads*
  ```matlab
  %channel for phase amplitude coupling
  %ch1 = ipsi HPC
  %ch2 = ipsi cortex
  %ch3 = contra HPC
  %ch4 = contral cortex
  ```
  
  ---
  
  # Discussion
  
  The data, every step of the way is
  - raw `.edf` files (~1.3G for a 24h recording) (question how quantized is this original signal? Probably i32 or i64)
  - `.dat` (~13G) files which contain 16bit quantization of 
    - raw x1
    - wavelet conv phase x20
    - wavelet conv power x20
  - `feature.mat` files with features (~7.3M)
  - `model` a trained model (aka `classification.mat`) (~21M), shared over many seizures
  - `predict.mat` predictions (...?)
  
  
  I think this codebase is going to be much more concise and easier to read in Python because we won't have to worry about indices anymore—which are among the major culprits of confusion. However, this shift will require some thinking before implementation since it may require us to make some adjustments to how we store data... especially the .dat indexing scheme. 
  
  I'm thinking of storing the `ops` in a `.toml` file for the python. 
  
  ### Cloud Compute
  
  In the below diagram, I think we fall into Virtual Machines (Build new -> Require full control -> VM), that we can build new. Yes, we will need to migrate a database, but I think we can treat the database as if it were new (am I correct in assuming this? -> ask SM)
  
  ![Azure Compute Choices](/Users/steve/boostnote/pics/unm/compute-choices-azure.png)
  
  ## Core Questions 
  These are questions I'm trying to find answers to, whos answers are central to my utility as an employee. 
  
  - How does the cloud computing & storage payment system work?
    - Is it tokens? How do tokens work? So far my best guess is that you pay some monthly rate for storage, and then you pay for the computing with a tokens system per in units of *cores* $\\times$ *hours* consumed.
  - What data format to use? 
    - It seems like the NWB data format is an attempt to unify neuroscience data formats. It might be short-sighted to keep using .dat files
  
  
  ## Other Questions 
  - After running `sm_MakeAll_getPowerPerChannel.m`—Is the original time-series preserved (as a channel) in the `.dat` files, or is it only the wavelet power filter bank? 
  - Is there any reason why we might want a different database type (e.g. SQL)? My sense is probably not. But need to look into this!
  - What should the process be for adding data to the database? Is this a first-class citizen (i.e. do we want to make this as simple as possible?)
  - Is there a plan to build an API for this database? So that "users" (general public) can run their own Featurizing + ML models?
  - Might there by some sense in computing the wavelet convolutions on the fly? So that we only have to compute it for training and testing windows instead of the whole dataset. 
  
  
  ## Todo
  Learn about: 
  - [ ] Docker, 
  - [ ] Kubernetes
  
  ## Loose threads (note to self)
  
  Look into `mscohere`: is it a matlab signal toolkit builtin? It's used in `sm_PredictIHKA_calcFeatures.m`
  
  ## Refs
  - [The NWB Software Ecosystem – Neurodata Without Borders](https://www.nwb.org/nwb-software/) 
  
  
  <br/><br/><br/><br/><br/>
  <br/><br/><br/><br/><br/>
  <br/><br/><br/><br/><br/>
  
  <style>
  fold {
    color:green; // i/o stuff
  }
  
  file {
    color:darkgreen; // i/o stuff
  }
  
  mldep {
    color:red; // matlab built-in dependency
  }
  
  heldep {
    color:darkred; // local dependency from helpers
  }
  
  buzdep {
    color:darkorange;
  }
  
  smdep {
    color:darkblue; // sam mckenzie dependency
  }
  
  var {
    color:pink;
  }
  
  red {
    color:red;
  }
  </style>
  
'''
linesHighlighted: []
isStarred: false
isTrashed: false
